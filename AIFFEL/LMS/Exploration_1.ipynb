{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "swiss-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 이미지를 28x28 사이즈로 resizing 해 주는 함수\n",
    "def resize_images(img_path):    \n",
    "    \n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aerial-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가위바위보 이미지를 각각 구분해 주는 함수\n",
    "def load_data(img_path, number_of_data=600):  \n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    \n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    return imgs, labels, idx  # 수행 된 train과 test의 갯수를 idx로 함께 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "danish-grocery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  images to be resized.\n",
      "200  images resized.\n",
      "200  images to be resized.\n",
      "200  images resized.\n",
      "200  images to be resized.\n",
      "200  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n"
     ]
    }
   ],
   "source": [
    "# rock, paper, sissor 별로 각각의 경로를 지정\n",
    "image_rock_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "image_paper_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "image_scissor_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/\"\n",
    "image_rock_test_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "image_scissor_test_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "image_paper_test_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "image_test_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "\n",
    "# train과 test에 필요한 rock, paper, sissor 이미지들을 resizing 하기 위한 함수 호출\n",
    "resize_images(image_rock_path)     # rock images for training\n",
    "resize_images(image_paper_path)    # paper images for training\n",
    "resize_images(image_scissor_path)  # sissor images for training\n",
    "resize_images(image_rock_test_path)      # rock images for testing\n",
    "resize_images(image_scissor_test_path)   # scissor images for testing\n",
    "resize_images(image_paper_test_path)     # paper images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wicked-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 600 입니다.\n",
      "TEST용 데이터(x_test)의 이미지 개수는 300 입니다.\n"
     ]
    }
   ],
   "source": [
    "# train과 test가 완료된 결과값 및 갯수를 return 받음\n",
    "(x_train, y_train, idx)=load_data(image_dir_path)\n",
    "print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "(x_test, y_test, idx)=load_data(image_test_path)\n",
    "print(\"TEST용 데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compatible-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 108,906\n",
      "Trainable params: 108,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters 값 설정\n",
    "n_channel_1 = 16   # 이미지에서 분석하고자하는 특징의 갯수를 지정\n",
    "n_channel_2 = 32\n",
    "n_dense=128        # 분류에 사용되는 뉴런의 숫자. 2의 배수로 조절해가며 결과값을 비교\n",
    "n_train_epoch=10   # 오차를 줄이기 위한 반복수행 횟수\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()  # 모델의 전반적인 정보를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "copyrighted-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용된 이미지의 Normalization(정규화) 진행. pixel의 경우 255.0으로 나눠줌.\n",
    "x_train_norm = x_train / 255.0\n",
    "x_test_norm = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "forward-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 칼라 이미지를 사용하기 위해 3 (RGB에 해당)을  argument로 입력. reshaping 진행\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산\n",
    "x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "impossible-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.9698\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f22814102d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras model을 compile 하기 위해 필요한 optimizer, loss, metrics를 입력\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 지정된 횟수만큼 training을 진행하여 accuracy를 확인\n",
    "model.fit(x_train_reshaped, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stainless-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 0s - loss: 2.6010 - accuracy: 0.8083\n",
      "test_loss: 2.600996494293213 \n",
      "test_accuracy: 0.8083333373069763\n"
     ]
    }
   ],
   "source": [
    "# Training이 된 model을 이용해 실제 test를 진행\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss)) # Test 결과 오차값을 보여줌\n",
    "print(\"test_accuracy: {}\".format(test_accuracy)) # Test 결과 accuracy 값을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-grade",
   "metadata": {},
   "source": [
    "**최종적으로 80% 이상의 accuracy를 가지는 결과를 얻음.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-accident",
   "metadata": {},
   "source": [
    "**회고**\n",
    ": 최종 결과값인 accuracy를 높이기 위해 hyperparameter들을 하나씩 조절해 본 결과 dense를 128로 하고 epoch는 10으로 했을 때 지금과같은 0.8 이상의 accuracy를 얻을 수 있었다.\n",
    "처음에 300장의 image만을 가지고 training을 진행한 뒤, 다른사람이 공유해준 300장의 image를 이용해 test 해봤을 때는 최대 0.6이 조금 넘는 정도의 accuracy를 보였다. 따라서 300장을 추가적으로 training 시킨뒤 다시 결과값을 도출해 봤을때 0.8 이상의 accuracy를 얻을 수 있었다. 이미지를 training 시킬 때 여러 각도, 조명, 배경 등의 요소들을 함께 training 시켜서 추후에 test 에 사용되는 이미지들의 outlier를 최소화 시키는게 무엇보다 중요하다고 생각한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
